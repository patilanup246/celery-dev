from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk, scan

from .logger import get_logger

__author__ = 'duydo'

logger = get_logger(__name__)
bulk_index = bulk


def get_user_posts_within_window(conn, index, doc_type, sns_name, user_sns_id, window_start, window_end,
                                 is_sponsored=False):
    query = {
        "query": {
            "bool": {
                "must": [
                    {
                        "term": {
                            "sns_name": sns_name
                        }
                    },
                    {
                        "term": {
                            "user_sns_id": user_sns_id
                        }
                    },
                    {
                        "range": {
                            "published_at": {
                                "from": window_start,
                                "to": window_end
                            }
                        }
                    },
                    {
                        "term": {
                            "is_sponsored": is_sponsored
                        }
                    }
                ]
            }

        }
    }

    return scan(conn, query=query, index=index, doc_type=doc_type, scroll='180m', size=500,
                request_timeout=3600)


def scan_docs_by_query(client=None, query=None, index=None, doc_type=None, returned_fields=None):
    return scan(
        client=client,
        query=query,
        index=index,
        doc_type=doc_type,
        fields=returned_fields
    )


class EsClientFactory(object):
    @staticmethod
    def create(hosts=None, **kwargs):
        """
        Create an ES client that has ability to inspect the cluster state to get a list of nodes upon startup, periodically and/or on failure,
        to retry to connect on connection failure.

        :param hosts: list of ES nodes the client connects to. Node should be a
            dictionary ({"host": "localhost", "port": 9200})
        :return: an instance of ElasticSearch class
        """
        from elasticsearch import Elasticsearch

        return Elasticsearch(
            hosts=hosts,
            retry_on_timeout=kwargs.get('retry_on_timeout', True),
            # sniff_on_start=kwargs.get('sniff_on_start', False),
            # sniff_on_connection_fail=kwargs.get('sniff_on_connection_fail', False),
            # sniff_timeout=kwargs.get('sniff_on_connection_fail', 60),
            timeout=kwargs.get('timeout', 60),
        )


class BulkIndexer(object):
    """The utility class provides bulk index operations: create, update and delete."""

    DEFAULT_BULK_SIZE = 1000

    def __init__(self, client=None, bulk_size=None, refresh=False):
        """
        :param client: the ElasticSearch instance
        :type client: ElasticSearch
        :param bulk_size: the bulk size. The default value is 1000
        :type bulk_size: int
        :param refresh: need to refresh the index after indexing?
        :type refresh: boolean
        """
        self.client = client or Elasticsearch()
        self.bulk_size = bulk_size or self.DEFAULT_BULK_SIZE
        self._bulk_count = 0
        self._bulk = []
        self.refresh = refresh
        self.default_index, self.default_doc_type = None, None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.flush()

    def __repr__(self):
        return '<BulkIndexer(%s)>' % self.client

    @classmethod
    def create(cls, hosts=None):
        return cls(client=EsClientFactory.create(hosts))

    @staticmethod
    def create_index_action(index, doc_type, doc_source, doc_id=None, **kwargs):
        """
        Helper method for creating index action.
        If doc_id is not provided, _id will be generated by Elasticsearch.
        """
        action = {
            '_op_type': 'index',
            '_index': index,
            '_type': doc_type,
            '_source': doc_source
        }

        if doc_id:
            action['_id'] = doc_id

        for key, value in kwargs.items():
            action[key] = value

        return action

    @staticmethod
    def create_update_action(index, doc_type, doc_id, doc, **kwargs):
        """Helper method for creating index action"""
        action = {
            '_op_type': 'update',
            '_index': index,
            '_type': doc_type,
            '_id': doc_id,
            'doc': doc
        }
        for key, value in kwargs.items():
            action[key] = value

        return action

    @staticmethod
    def create_delete_action(index, doc_type, doc_id):
        """Helper method for creating delete action"""
        action = {
            '_op_type': 'delete',
            '_index': index,
            '_type': doc_type,
            '_id': doc_id,
        }
        return action

    def _add_to_bulk(self, action):
        self._bulk.append(action)
        self._bulk_count += 1

    def default_index(self, index=None, doc_type=None):
        self.default_index = index
        self.default_doc_type = doc_type

    def add_action(self, action):
        """
        Add an action to bulk.
        :arg action: the action is in the format as returned by elasticsearch search API, for example:
        {
            '_index': 'index-name',
            '_type': 'document',
            '_id': 42,
            '_parent': 5,
            '_ttl': '1d',
            '_source': {
                ...
            }
        }
        """
        self._add_to_bulk(action)
        self._process_bulk_if_available()

    def add_index_action(self, index=None, doc_type=None, doc_source=None, doc_id=None, **kwargs):
        self.add_action(
            self.create_index_action(index or self.default_index, doc_type or self.default_doc_type, doc_source, doc_id,
                                     **kwargs)
        )

    def add_update_action(self, index=None, doc_type=None, doc_id=None, doc=None, **kwargs):
        self.add_action(
            self.create_update_action(index or self.default_index, doc_type or self.default_doc_type, doc_id, doc,
                                      **kwargs))

    def add_delete_action(self, index=None, doc_type=None, doc_id=None):
        self.add_action(
            self.create_delete_action(index or self.default_index, doc_type or self.default_doc_type, doc_id))

    def flush(self):
        """Forces all actions in bulk are indexed"""
        if self._bulk_count > 0:
            self.do_index()

    def before_index(self, actions):
        pass

    def after_index(self, actions):
        pass

    def do_index(self):
        success, failed = 0, 0
        try:
            logger.info('indexing: %d...', self._bulk_count)
            self.before_index(self._bulk)
            success, failed = bulk_index(
                client=self.client,
                actions=self._bulk,
                chunk_size=self._bulk_count,
                refresh=self.refresh,
                stats_only=True
            )
            if failed:
                logger.error(str(failed))
            logger.info('success: %d, failed: %d', success, failed)
            self.after_index(self._bulk)
            self._reset_bulk()
        except Exception as e:
            logger.exception(e)

        return success, failed

    def _reset_bulk(self):
        self._bulk_count = 0
        self._bulk = []

    def _bulk_available(self):
        return self._bulk_count >= self.bulk_size

    def _process_bulk_if_available(self):
        if self._bulk_available():
            self.do_index()
